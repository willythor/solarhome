A NUMBER of proposals have been advanced in
recent years for the development of “general sys-
tems theory” which, abstracting from properties
peculiar to physical, biological, or social systems,
would be applicable to all of them. *1 We might
well feel that, while the goal is laudable, systems
of such diverse kinds could hardly be expected to
have any nontrivial properties in common. Meta-
phor and analogy can be helpful, or they can be
misleading. All depends on whether the similari-
ties the metaphor captures are significant or su-
perficial.
It may not be entirely vain, however, to search
for common properties among diverse kinds of
complex systems. The ideas that go by the name
of cybernetics constitute, if not a theory, at least a
point of view that has been proving fruitful over
a wide range of applications. 2 It has been useful
to look at the behavior of adaptive systems in
terms of the concepts of feedback and homeosta-
See especially the yearbooks of the Society for Gen-
eral Systems Research. Prominent among the exponents
of general systems theory are L. von Bertalanffy, K.
Boulding, R. W. Gerard, and J. G. Miller. For a more
skeptical view—perhaps too skeptical in the light of the
present discussion—see H. A. Simon and A. Newell,
Models: their uses and limitations, in L. D. White, ed.,
The state of the social sciences, 66-83, Chicago, Univ. of
Chicago
Press, 1956.
N. Wiener, Cybernetics, New York, Wiley, 1948. For
an imaginative forerunner, see A. J. Lotka, Elements of
mathematical biology, New York, Dover Publications,
1951, first published in 1924 as Elements of physical
biology.
sis, and to analyze adaptiveness in terms of the
theory of selective information. 3 The ideas of
feedback and information provide a frame of ref-
erence for viewing a wide range of situations,
just as do the ideas of evolution, or relativism, of
axiomatic method, and of operationalism.
In this essay I should like to report on some
things we have been learning about particular
kinds of complex systems encountered in the be-
havioral sciences. The developments I shall dis-
cuss arose in the context of specific phenomena,
but the theoretical formulations themselves make
little reference to details of structure. Instead they
refer primarily to the complexity of the systems
under view without specifying the exact content
of that complexity. Because of their abstractness,
the theories may have relevance—application
would be too strong a term— to other kinds of
complex systems that are observed in the social,
biological, and physical sciences.
In recounting these developments, I shall avoid
technical detail, which can generally be found
elsewhere. I shall describe each theory in the par-
ticular context in which it arose. Then, I shall cite
some examples of complex systems, from areas
of science other than the initial application, to
which the theoretical framework appears rele-
vant. In doing so, I shall make reference to areas
of knowledge where I am not expert—perhaps
not even literate. I feel quite comfortable in doing
so before the members of this society, represent-
ing as it does the whole span of the scientific and
scholarly endeavor. Collectively you will have
little difficulty, I am sure, in distinguishing in-
stances based on idle fancy or sheer ignorance
from instances that cast some light on the ways in
which complexity exhibits itself wherever it is
found in nature. I shall leave to you the final
judgment of relevance in your respective fields.
I shall not undertake a formal definition of
3
C. Shannon and W. Weaver, The mathematical the-
ory of communication, Urbana, Univ. of Illinois Press,
1949; W. R. Ashby, Design for a brain, New York,
Wiley, 1952.
PROCEEDINGS OF THE AMERICAN PHILOSOPHICAL SOCIETY, VOL. 106, NO. 6, DECEMBER, 1962 . Reprinted with permission.VOL. 106, NO. 6, 1962]
THE ARCHITECTURE OF COMPLEXITY
“complex systems.” 4 Roughly, by a complex sys-
tem I mean one made up of a large number of
parts that interact in a nonsimple way. In such
systems, the whole is more than the sum of the
parts, not in an ultimate, metaphysical sense, but
in the important pragmatic sense that, given the
properties of the parts and the laws of their inter-
action, it is not a trivial matter to infer the proper-
ties of the whole. In the face of complexity, an
in-principle reductionist may be at the same time
a pragmatic holist. 5
The four sections that follow discuss four as-
pects of complexity. The first offers some com-
ments on the frequency with which complexity
takes the form of hierarchy —the complex sys-
tem being composed of subsystems that, in turn,
have their own subsystems, and so on. The sec-
ond section theorizes about the relation between
the structure of a complex system and the time
required for it to emerge through evolutionary
processes; specifically, it argues that hierarchic
systems will evolve far more quickly than nonhi-
erarchic systems of comparable size. The third
section explores the dynamic properties of hier-
archically organized systems and shows how they
can be decomposed into subsystems in order to
analyze their behavior. The fourth section exam-
ines the relation between complex systems and
their descriptions.
Thus, the central theme that runs through my
remarks is that complexity frequently takes the
form of hierarchy, and that hierarchic systems
have some common properties that are independ-
ent of their specific content. Hierarchy, I shall
argue, is one of the central structural schemes
that the architect of complexity uses.
4
W. Weaver, in: Science and complexity, American
Scientist 36: 536 1948, has distinguished two kinds of
complexity, disorganized and organized. We shall be
concerned
primarily with organized complexity.
5
See also John R. Platt, Properties of large molecules
that go beyond the properties of their chemical sub-
groups, Journ. Theoret. Biol. 1: 342-358 (1961). Since
the reductionism-holism issue is a major cause de guerre
between scientists and humanists, perhaps we might even
hope that peace could be negotiated between the two
cultures along the lines of the compromise just sug-
gested. As 1 go along, I shall have a little to say about
complexity in the arts as well as in the natural sciences. 1
must emphasize the pragmatism of my holism to distin-
guish it sharply from the position taken by W. M. El-
sasser in The physical foundation of biology, New York,
Pergamon Press, 1958.
468
HIERARCHIC SYSTEMS
By a hierarchic system, or hierarchy, I mean a
system that is composed of interrelated subsys-
tems, each of the latter being, in turn, hierarchic
in structure until we reach some lowest level of
elementary subsystem. In most systems in nature,
it is somewhat arbitrary as to where we leave off
the partitioning and what subsystems we take as
elementary. Physics makes much use of the con-
cept of “elementary particle,” although particles
have a disconcerting tendency not to remain ele-
mentary very long. Only a couple of generations
ago, the atoms themselves were elementary parti-
cles; today, to the nuclear physicist they are
complex systems. For certain purposes of astron-
omy, whole stars, or even galaxies, can be re-
garded as elementary subsystems. In one kind of
biological research, a cell may be treated as an
elementary subsystem; in another, a protein
molecule; in still another, an amino acid residue.
Just why a scientist has a right to treat as ele-
mentary a subsystem that is in fact exceedingly
complex is one of the questions we shall take up.
For the moment, we shall accept the fact that sci-
entists do this all the time and that, if they are
careful scientists, they usually get away with it.
Etymologically, the word “hierarchy” has had a
narrower meaning than I am giving it here. The
term has generally been used to refer to a com-
plex system in which each of the subsystems is
subordinated by an authority relation to the sys-
tem it belongs to. More exactly, in a hierarchic
formal organization, each system consists of a
“boss” and a set of subordinate subsystems. Each
of the subsystems has a “boss” who is the imme-
diate subordinate of the boss of the system. We
shall want to consider systems in which the rela-
tions among subsystems are more complex than
in the formal organizational hierarchy just de-
scribed. We shall want to include systems in
which there is no relation of subordination among
subsystems. (In fact, even in human organiza-
tions, the formal hierarchy exists only on paper;
the real flesh-and-blood organization has many
interpart relations other than the lines of formal
authority.) For lack of a better term, I shall use
“hierarchy” in the broader sense introduced in the
previous paragraphs, to refer to all complex sys-
tems analyzable into successive sets of subsys-
tems, and speak of “formal hierarchy” when I
want to refer to the more specialized concept. 6
6
The mathematical term “partitioning” will not do for
what I call here a hierarchy; for the set of subsystems,HERBERT A. SIMON
469
SOCIAL SYSTEMS
I have already given an example of one kind of
hierarchy that is frequently encountered in the
social sciences: a formal organization. Business
firms, governments, universities all have a clearly
visible parts-within-parts structure. But formal
organizations are not the only, or even the most
common, kind of social hierarchy. Almost all
societies have elementary units called families,
which may be grouped into villages or tribes, and
these into larger groupings, and so on. If we
make a chart of social interactions, of who talks
to whom, the clusters of dense interaction in the
chart will identify a rather well-defined hierarchic
structure. The groupings in this structure may be
defined operationally by some measure of fre-
quency of interaction in this sociometric matrix.
BIOLOGICAL AND PHYSICAL SYSTEMS
The hierarchical structure of biological systems
is a familiar fact. Taking the cell as the building
block, we find cells organized into tissues, tissues
into organs, organs into systems. Moving down-
ward from the cell, well-defined subsystems—for
example, nucleus, cell membrane, micro-somes,
mitochondria, and so on—have been identified in
animal cells.
The hierarchic structure of many physical sys-
tems is equally clear-cut. I have already men-
tioned the two main series. At the microscopic
level we have elementary particles, atoms, mole-
cules, macromolecules. At the macroscopic level
we have satellite systems, planetary systems, gal-
axies. Matter is distributed throughout space in a
strikingly nonuniform fashion. The most nearly
random distributions we find, gases, are not ran-
dom distributions of elementary particles but
random distributions of complex systems, i.e.
molecules.
A considerable range of structural types is sub-
sumed under the term hierarchy as I have defined
it. By this definition, a diamond is hierarchic, for
it is a crystal structure of carbon atoms that can
be further decomposed into protons, neutrons,
and electrons. However, it is a very “flat” hierar-
chy, in which the number of first-order subsys-
tems belonging to the crystal can be indefinitely
large. A volume of molecular gas is a flat hierar-
and the successive subsets in each of these defines the
partitioning, independently of any systems of relations
among the subsets. By “hierarchy” 1 mean the partition-
ing in conjunction with the relations that hold among its
parts.
[ PROC. AMER. PHIL. SOC
chy in the same sense. In ordinary usage, we tend
to reserve the word “hierarchy” for a system that
is divided into a small or moderate number of
subsystems, each of which may be further subdi-
vided. Hence, we do not ordinarily think of or
refer to a diamond or a gas as a hierarchic struc-
ture. Similarly, a linear polymer is simply a
chain, which may be very long, of identical sub-
parts, the monomers. At the molecular level it is
a very flat hierarchy.
In discussing formal organizations, the number
of subordinates who report directly to a single boss
is called his span of control. I will speak analo-
gously of the span of a system, by which I shall
mean the number of subsystems into which it is
partitioned. Thus, a hierarchic system is flat at a
given level if it has a wide span at that level. A
diamond has a wide span at the crystal level, but
not at the next level down, the molecular level.
In most of our theory construction in the fol-
lowing sections we shall focus our attention on
hierarchies of moderate span, but from time to
time I shall comment on the extent to which the
theories might or might not be expected to apply
to very flat hierarchies.
There is one important difference between the
physical and biological hierarchies, on the one
hand, and social hierarchies, on the other. Most
physical and biological hierarchies are described
in spatial terms. We detect the organelles in a cell
in the way we detect the raisins in a cake—they
are “visibly” differentiated substructures local-
ized spatially in the larger structure. On the other
hand, we propose to identify social hierarchies
not by observing who lives close to whom but by
observing who interacts with whom. These two
points of view can be reconciled by defining hi-
erarchy in terms of intensity of interaction, but
observing that in most biological and physical
systems relatively intense interaction implies rela-
tive spatial propinquity. One of the interesting
characteristics of nerve cells and telephone wires
is that they permit very specific strong interac-
tions at great distances. To the extent that interac-
tions are channeled through specialized commu-
nications and transportation systems, spatial pro-
pinquity becomes less determinative of structure.
SYMBOLIC SYSTEMS
One very important class of systems has been
omitted from my examples thus far: systems of
human symbolic production. A book is a hierar-
chy in the sense in which I am using that term. It
is generally divided into chapters, the chaptersVOL. 106, NO. 6, 1962]
THE ARCHITECTURE OF COMPLEXITY
into sections, the sections into paragraphs, the
paragraphs into sentences, the sentences into
clauses and phrases, the clauses and phrases into
words. We may take the words as our elementary
units, or further subdivide them, as the linguist
often does, into smaller units. ]f the book is nar-
rative in character, it may divide into “episodes”
instead of sections, but divisions there will be.
The hierarchic structure of music, based on such
units as movements, parts, themes, phrases, is well
known. The hierarchic structure of products of
the pictorial arts is more difficult to characterize,
but I shall have something to say about it later.
THE EVOLUTION OF COMPLEX SYSTEMS
Let me introduce the topic of evolution with a
parable. There once were two watchmakers,
named Hora and Tempus, who manufactured
very fine watches. Both of them were highly re-
garded, and the phones in their workshops rang
frequently—new customers were constantly call-
ing them. However, Hora prospered, while Tem-
pus became poorer and poorer and finally lost his
shop. What was the reason?
The watches the men made consisted of about
1,000 parts each. Tempus had so constructed his
that if he had one partly assembled and had to put
it down—to answer the phone, say—it immedi-
ately fell to pieces and had to be reassembled
from the elements. The better the customers liked
his watches, the more they phoned him and the
more difficult it became for him to find enough
uninterrupted time to finish a watch.
The watches that Hora made were no less
complex than those of Tempus. But he had de-
signed them so that he could put together subas-
semblies of about ten elements each. Ten of these
subassemblies, again, could be put together into a
larger subassembly; and a system of ten of the
latter subassemblies constituted the whole watch.
Hence, when Hora had to put down a partly as-
sembled watch in order to answer the phone, he
lost only a small part of his work, and he assem-
bled his watches in only a fraction of the man-
hours it took Tempus.
It is rather easy to make a quantitative analysis
of the relative difficulty of the tasks of Tempus
and Hora: Suppose the probability that an inter-
ruption will occur while a part is being added to
an incomplete assembly is p. Then the probability
that Tempus can complete a watch he has started
without interruption is (1 – p) 1000 —a very small
number unless p is 0.001 or less. Each interrup-
470
tion will cost, on the average, the time to assem-
ble 1/p parts (the expected number assembled
before interruption). On the other hand, Hora has
to complete 111 subassemblies of ten parts each.
The probability that he will not be interrupted
while completing any one of these is (1 — p) 10 ,
and each interruption will cost only about the
time required to assemble five parts. 7
Now if p is about 0.01—that is, there is one
chance in a hundred that either watchmaker will
be interrupted while adding any one part to an
assembly—then a straightforward calculation
shows that it will take Tempus, on the average,
about four thousand times as long to assemble a
watch as Hora.
We arrive at the estimate as follows:
1. Hora must make 111 times as many complete
assemblies per watch as Tempus; but
2. Tempus will lose on the average 20 times as
much work for each interrupted assembly as
Hora [100 parts, on the average, as against
5]; and
3. Tempus will complete an assembly only 44
times per million attempts (0.99 1000 = 44 ×
10 -6 ), while Hora will complete nine out of
ten (0.99 10 = 9 × 10 -1 ). Hence Tempus will
have to make 20,000 as many attempts per
completed assembly as Hora. (9 × 10 -1 )/(44 ×
10 -6 ) = 2 × 10 4 . Multiplying these three ra-
tios, we get
1/111 × 100/× 0.99 10 /0.99 1000
= 1/111 × 20 × 20,000 ~ 4,000.
7
The speculations on speed of evolution were first sug-
gested by H. Jacobson’s application of information theory to
estimating the time required for biological evolution. See
his paper Information, reproduction, and the origin of
life, in American Scientist, 43: 119-127, January 1955.
From thermodynamic considerations it is possible to
estimate the amount of increase in entropy that occurs
when a complex system decomposes into its elements.
( See for example, R. B. Setlow and E. C. Pollard, Molecular
biophysics , 63-65, Reading, Mass., Addison-Wesley Pub-
lishing Co., 1962, and references cited there.) But en-
tropy is the logarithm of a probability, hence information,
the negative of entropy, can be interpreted as the loga-
rithm of the reciprocal of the probability—the “improb-
ability,” so to speak. The essential idea in Jacobson’s
model is that the expected time required for the system to
reach a particular state is inversely proportional to the
probability of the state—hence it increases exponentially
with the amount of information (negentropy) of the state.
Following this line of argument, but not introducing
the notion of levels and stable subassemblies, Jacobson
arrived at estimates of the time required for evolution so
large as to make the event rather improbable. Our analy-
sis, carried through in the same way, but with attention to
the stable intermediate forms, produces very much
smaller estimates.HERBERT A. SIMON
471
BIOLOGICAL EVOLUTION
What lessons can we draw from our parable for
biological evolution? Let us interpret a partially
completed sub-assembly of k elementary parts as
the coexistence of k parts in a small volume—
ignoring their relative orientations.
The model assumes that parts are entering the
volume at a constant rate, but that there is a con-
stant probability, p, that the part will be dispersed
before another is added, unless the assembly
reaches a stable state. These assumptions are not
particularly realistic. They undoubtedly underes-
timate the decrease in probability of achieving
the assembly with increase in the size of the as-
sembly. Hence the assumptions understate—
probably by a large factor—the relative advan-
tage of a hierarchic structure.
Although we cannot, therefore, take the nu-
merical estimate seriously, the lesson for biologi-
cal evolution is quite clear and direct. The time
required for the evolution of a complex form
from simple elements depends critically on the
numbers and distribution of potential intermedi-
ate stable forms. In particular, if there exists a
hierarchy of potential stable “subassemblies,”
with about the same span, s, at each level of the
hierarchy, then the time required for a subassem-
bly can be expected to be about the same at each
level—that is, proportional to 1/(1 — p) s . The
time required for the assembly of a system of n
elements will be proportional to log s n, that is, to
the number of levels in the system. One would
say—with more illustrative than literal intent—
that the time required for the evolution of multi-
celled organisms from single-celled organisms
might be of the same order of magnitude as the
time required for the evolution of single-celled
organisms from macromolecules. The same ar-
gument could be applied to the evolution of pro-
teins from amino acids, of molecules from atoms,
of atoms from elementary particles.
A whole host of objections to this oversimpli-
fied scheme will occur, I am sure, to every work-
ing biologist, chemist, and physicist. Before turn-
ing to matters I know more about, I shall mention
three of these problems, leaving the rest to the
attention of the specialists.
First, in spite of the overtones of the watch-
maker parable, the theory assumes no teleologi-
cal mechanism. The complex forms can arise
from the simple ones by purely random proc-
esses. (I shall propose another model in a mo-
ment that shows this clearly.) Direction is pro-
vided to the scheme by the stability of the com-
[ PROC. AMER. PHIL. SOC
plex forms, once these come into existence. But
this is nothing more than survival of the fittest—
i.e., of the stable.
Second, not all large systems appear hierarchi-
cal. For example, most polymers—such as ny-
lon—are simply linear chains of large numbers of
identical components, the monomers. However,
for present purposes we can simply regard such a
structure as a hierarchy with a span of one—the
limiting case. For a chain of any length repre-
sents a state of relative equilibrium. 8
Third, the evolution of complex systems from
simple elements implies nothing, one way or the
other, about the change in entropy of the entire
system. If the process absorbs free energy, the
complex system will have a smaller entropy than
the elements; if it releases free energy, the oppo-
site will be true. The former alternative is the one
that holds for most biological systems, and the
net inflow of free energy has to be supplied from
the sun or some other source if the second law of
thermodynamics is not to be violated. For the
evolutionary process we are describing, the equi-
libria of the intermediate states need have only
local and not global stability, and they may be
stable only in the steady state—that is, as long as
there is an external source of free energy that
may be drawn upon. 9
Because organisms are not energetically closed
systems, there is no way to deduce the direction,
much less the rate, of evolution from classical
thermodynamic considerations. All estimates in-
dicate that the amount of entropy, measured in
physical units, involved in the formation of a
one-celled biological organism is trivially
small—about –10 -11 cal/degree. 10 The “improb-
ability” of evolution has nothing to do with this
quantity of entropy, which is produced by every
bacterial cell every generation. The irrelevance of
8
There is a well-developed theory of polymer size,
based on models of random assembly. See, for example,
P. J. Flory, Principles of polymer chemistry, ch. 8, Ithaca,
Cornell Univ. Press, 1953. Since all subassemblies in the
polymerization theory are stable, limitation of molecular
growth depends on “poisoning” of terminal groups by
impurities or formation of cycles rather than upon disrup-
tion
of partially formed chains.
9
This point has been made many times before, but it
cannot be emphasized too strongly. For further discus-
sion, see Setlow and Pollard, op cit., 49-64; E. Schrod-
inger, What Is life? Cambridge Univ. Press, 1945; and H.
Linschitz, The information content of a bacterial cell, in
H. Questler ed., Information theory in biology, 251-262,
Urbana, Univ. of Illinois Press, 1953.
10
See Linschitz, op. cit. This quantity, 10 -11 cal/degree,
corresponds to about 10 13 bits of information.VOL. 106, NO. 6, 1962]
THE ARCHITECTURE OF COMPLEXITY
quantity of information, in this sense, to speed of
evolution can also be seen from the fact that ex-
actly as much information is required to “copy” a
cell through the reproductive process as to pro-
duce the first cell through evolution.
The effect of the existence of stable intermedi-
ate forms exercises a powerful effect on the evo-
lution of complex forms that may be likened to
the dramatic effect of catalysts upon reaction
rates and steady-state distribution of reaction
products in open systems. 11 In neither case does
the entropy change provide us with a guide to
system behavior.
PROBLEM SOLVING AS NATURAL SELECTION
Let us turn now to some phenomena that have
no obvious connection with biological evolution:
human problem-solving processes. Consider, for
example, the task of discovering the proof for a
difficult theorem. The process can be—and often
has been—described as a search through a maze.
Starting with the axioms and previously proved
theorems, various transformations allowed by the
rules of the mathematical systems are attempted,
to obtain new expressions. These are modified in
turn until, with persistence and good fortune, a
sequence or path of transformations is discovered
that leads to the goal.
The process ordinarily involves much trial and
error. Various paths are tried; some are aban-
doned, others are pushed further. Before a solu-
tion is found, many paths of the maze may be
explored. The more difficult and novel the prob-
lem, the greater is likely to be the amount of trial
and error required to find a solution. At the same
time, the trial and error is not completely random
or blind; it is, in fact, rather highly selective. The
new expressions that are obtained by transform-
ing given ones are examined to see whether they
represent progress toward the goal. Indications of
progress spur further search in the same direc-
tion; lack of progress signals the abandonment of
a line of search. Problem solving requires selec-
tive trial and error. 12
11
See H. Kacser, Some physico-chemical aspects of
biological organization, Appendix, pp. 191-249, in C. H.
Waddington, The strategy of the genes, London, George
Allen
& Unwin, 1957.
12
See A. Newell, J. C. Shaw, and H. A. Simon, Em-
pirical explorations of the logic theory machine, Pro-
ceedings of the 1957 Western Joint Computer Confer-
ence, February 1957, New York: Institute of Radio Engi-
neers; Chess-playing programs and the problem of com-
plexity, IBM Journal of Research and Development 2:
320-335, October 1958; and for a similar view of prob-
472
A little reflection reveals that cues signaling
progress play the same role in the problem-
solving process that stable intermediate forms
play in the biological evolutionary process. In
fact, we can take over the watchmaker parable
and apply it also to problem solving. In problem
solving, a partial result that represents recogniz-
able progress toward the goal plays the role of a
stable subassembly.
Suppose that the task is to open a safe whose
lock has 10 dials, each with 100 possible settings,
numbered from 0 to 99. How long will it take to
open the safe by a blind trial-and-error search for
the correct setting? Since there are 100 10 possible
settings, we may expect to examine about one
half of these, on the average, before finding the
correct one—that is, 50 billion billion settings.
Suppose, however, that the safe is defective, so
that a click can be heard when any one dial is
turned to the correct setting. Now each dial can
be adjusted independently and does not need to
be touched again while the others are being set.
The total number of settings that have to be tried
is only 10 x 50, or 500. The task of opening the
safe has been altered, by the cues the clicks pro-
vide, from a practically impossible one to a triv-
ial one. 13
A considerable amount has been learned in the
past five years about the nature of the mazes that
represent common human problem-solving tasks—
proving theorems, solving puzzles, playing chess,
making investments, balancing assembly lines, to
mention a few. All that we have learned about
these mazes points to the same conclusion: that
human problem solving, from the most blundering
to the most insightful, involves nothing more than
varying mixtures of trial and error and selectivity.
The selectivity derives from various rules of
lem solving, W. R. Ashby, Design for an intelligence
amplifier, 215-233 in C. E. Shannon and J. McCarthy,
Automata
studies, Princeton, Princeton Univ. Press, 195).
13
The clicking safe example was supplied by D. P.
Simon. Ashby, op. cit., 230, has called the selectivity
involved in situations of this kind “selection by compo-
nents.” The even greater reduction in time produced by
hierarchization in the clicking safe example, as compared
with the watchmaker’s metaphor, is due to the fact that a
random search for the correct combination is involved in
the former case, while in the latter the parts come to-
gether in the right order. It is not clear which of these
metaphors provides the better model for biological evolu-
tion, but we may be sure that the watchmaker’s metaphor
gives an exceedingly conservative estimate of the savings
due to hierarchization. The safe may give an excessively
high estimate because it assumes all possible arrange-
ments of the elements to be equally probable.473
HERBERT A. SIMON
thumb, or heuristics, that suggest which paths
should be tried first and which leads are promis-
ing. We do not need to postulate processes more
sophisticated than those involved in organic evo-
lution to explain how enormous problem mazes
are cut down to quite reasonable size. 14
THE SOURCES OF SELECTIVITY
When we examine the sources from which the
problem-solving system, or the evolving system,
as the case may be, derives its selectivity, we dis-
cover that selectivity can always be equated with
some kind of feedback of information from the
environment.
Let us consider the case of problem solving first.
There are two basic kinds of selectivity. One we
have already noted: various paths are tried out,
the consequences of following them are noted, and
this information is used to guide further search.
In the same way, in organic evolution, various
complexes come into being, at least evanescently,
and those that are stable provide new building
blocks for further construction. It is this informa-
tion about stable configurations, and not free en-
ergy or negentropy from the sun, that guides the
process of evolution and provides the selectivity
that is essential to account for its rapidity.
The second source of selectivity in problem
solving is previous experience. We see this par-
ticularly clearly when the problem to be solved is
similar to one that has been solved before. Then,
by simply trying again the paths that led to the
earlier solution, or their analogues, trial-and-error
search is greatly reduced or altogether eliminated.
What corresponds to this latter kind of informa-
tion in organic evolution? The closest analogue is
reproduction. Once we reach the level of self-
reproducing systems, a complex system, when it
has once been achieved, can be multiplied indefi-
nitely. Reproduction, in fact, allows the inheri-
tance of acquired characteristics, but at the level
of genetic material, of course; that is, only char-
acteristics acquired by the genes can be inherited.
We shall return to the topic of reproduction in the
final section of this paper.
ON EMPIRES AND EMPIRE BUILDING
We have not exhausted the categories of com-
plex systems to which the watchmaker argument
can reasonably be applied. Philip assembled his
14
A. Newell and H. A. Simon, Computer simulation of
human thinking, Science 134: 2011-2017, December 22,
1961.
[ PROC. AMER. PHIL. SOC
Macedonian empire and gave it to his son, to be
later combined with the Persian subassembly and
others into Alexander’s greater system. On Alex-
ander’s death, his empire did not crumble to dust
but fragmented into some of the major subsys-
tems that had composed it.
The watchmaker argument implies that if one
would be Alexander, one should be born into a
world where large stable political systems al-
ready exist. Where this condition was not ful-
filled, as on the Scythian and Indian frontiers,
Alexander found empire building a slippery busi-
ness. So too, T. E. Lawrence’s organizing of the
Arabian revolt against the Turks was limited by
the character of his largest stable building blocks,
the separate, suspicious desert tribes.
The profession of history places a greater value
upon the validated particular fact than upon ten-
dentious generalization. I shall not elaborate
upon my fancy, therefore, but shall leave it to
historians to decide whether anything can be
learned for the interpretation of history from an
abstract theory of hierarchic complex systems.
CONCLUSION: THE EVOLUTIONARY EXPLANATION
OF HIERARCHY
We have shown thus far that complex systems
will evolve from simple systems much more rap-
idly if there are stable intermediate forms than if
there are not. The resulting complex forms in the
former case will be hierarchic. We have only to
turn the argument around to explain the observed
predominance of hierarchies among the complex
systems nature presents to us. Among possible
complex forms, hierarchies are the ones that have
the time to evolve. The hypothesis that complex-
ity will be hierarchic makes no distinction among
very flat hierarchies, like crystals and tissues and
polymers, and the intermediate forms. Indeed, in
the complex systems we encounter in nature, ex-
amples of both forms are prominent. A more
complete theory than the one we have developed
here would presumably have something to say
about the determinants of width of span in these
systems.
NEARLY DECOMPOSABLE SYSTEMS
In hierarchic systems, we can distinguish be-
tween the interactions among subsystems, on the
one hand, and the interactions within subsystems—
i.e., among the parts of those subsystems—on the
other. The interactions at the different levels may
be, and often will be, of different orders of mag-THE ARCHITECTURE OF COMPLEXITY
VOL. 106, NO. 6, 1962]
A1 A2 A3 B1 B2 C1 C2 C3
A1 — 100 — 2
A2 100 — 100 1
A2 — 100 — — —
1
2 —
—
— —
—
— —
—
—
1
1 —
2
B1 2
B2 — 1
1 —
2 — 100 2
100 — —
C1 —
C2 —
C3 — —
—
— —
—
— 2
1
—
—
—
2
— 100 —
100 — 100
— 100 —
F IG . 1. A Hypothetical Nearly Decomposable Sys-
tem. In terms of the heat-exchange example of
the text, Al, A2, and A3 may be interpreted as
cubicles in one room, Bl and B2 as cubicles in a
second room, and Cl, C2, and C3 as cubicles in a
third. The matrix entries then are the heat diffu-
sion coefficients between cubicles.
Al
A2
A3
Bl
B2
Cl
C2
C3
nitude. In a formal organization there will gener-
ally be more interaction, on the average, between
two employees who are members of the same
department than between two employees from
different departments. In organic substances, in-
termolecular forces will generally be weaker than
molecular forces, and molecular forces weaker
than nuclear forces.
In a rare gas, the intermolecular forces will be
negligible compared to those binding the molecules —
we can treat the individual particles, for many
purposes, as if they were independent of each
other. We can describe such a system as decom-
posable into the subsystems comprised of the indi-
vidual particles. As the gas becomes denser, mo-
lecular interactions become more significant. But
over some range, we can treat the decomposable
case as a limit and as a first approximation. We
can use a theory of perfect gases, for example, to
describe approximately the behavior of actual
gases if they are not too dense. As a second ap-
proximation, we may move to a theory of nearly
decomposable systems, in which the interactions
among the subsystems are weak, but not negligible.
At least some kinds of hierarchic systems can be
approximated successfully as nearly decomposable
systems. The main theoretical findings from the
approach can be summed up in two propositions:
474
(a) in a nearly decomposable system, the short-
run behavior of each of the component subsys-
tems is approximately independent of the short-
run behavior of the other components; (b) in the
long run, the behavior of any one of the compo-
nents depends in only an aggregate way on the
behavior of the other components.
Let me provide a very concrete simple example
of a nearly decomposable system. 15 Consider a
building whose outside walls provide perfect
thermal insulation from the environment. We
shall take these walls as the boundary of our sys-
tem. The building is divided into a large number
of rooms, the walls between them being good,
but not perfect, insulators. The walls between
rooms are the boundaries of our major subsys-
tems. Each room is divided by partitions into a
number of cubicles, but the partitions are poor
insulators. A thermometer hangs in each cubicle.
Suppose that at the time of our first observation
of the system there is a wide variation in tem-
perature from cubicle to cubicle and from room
to room—the various cubicles within the building
are in a state of thermal disequilibrium. When we
take new temperature readings several hours
later, what shall we find? There will be very little
variation in temperature among the cubicles
within each single room, but there may still be
large temperature variations among rooms. When
we take readings again several days later, we find
an almost uniform temperature throughout the
building; the temperature differences among
rooms have virtually disappeared.
We can describe the process of equilibration
formally by setting up the usual equations of heat
flow. The equations can be represented by the
matrix of their coefficients, r ij , where r ij is the
rate at which heat flows from the ith cubicle to
the jth cubicle per degree difference in their tem-
peratures. If cubicles i and j do not have a com-
mon wall, r ij will be zero. If cubicles i and j have
a common wall and are in the same room, r ij will
be large. If cubicles i and j are separated by the
15
This discussion of near decomposability is based
upon H. A. Simon and A. Ando, Aggregation of vari-
ables in dynamic systems, Econometrics 29: 111-138,
April 1961. The example is drawn from the same source,
117-118. The theory has been further developed and ap-
plied to a variety of economic and political phenomena
by Ando and F. M. Fisher. See F. M. Fisher, On the cost
of approximate specification in simultaneous equation
estimation, Econometrica 29: 139-170, April 1961, and
F. M. Fisher and A. Ando, Two theorems on Ceteris
Paribus in the analysis of dynamic systems, American
Political Science Review 61: 103-113, March 1962.475
HERBERT A. SIMON
[ PROC. AMER. PHIL. SOC
wall of a room, r ij will be nonzero but small.
Hence, by grouping together all the cubicles that
are in the same room, we can arrange the matrix
of coefficients so that all its large elements lie
inside a string of square submatrices along the
main diagonal. All the elements outside these
diagonal squares will be either zero or small (see
Figure 7). We may take some small number, ,
as the upper bound of the extra-diagonal ele-
ments. We shall call a matrix having these prop-
erties a nearly decomposable matrix.
Now it has been proved that a dynamic system
that can be described by a nearly decomposable
matrix has the properties, stated earlier, of a
nearly decomposable system. In our simple ex-
ample of heat flow this means that in the short
run each room will reach an equilibrium tempera-
ture (an average of the initial temperatures of its
offices) nearly independently of the others; and
that each room will remain approximately in a
state of equilibrium over the longer period during
which an over-all temperature equilibrium is be-
ing established throughout the building. After the
intraroom short-run equilibria have been reached,
a single thermometer in each room will be ade-
quate to describe the dynamic behavior of the
entire system—separate thermometers in each
cubicle will be superfluous. are associated, in general, with the main flows of
raw materials and semifinished products within
and between industries. An input-output matrix
of the economy, giving the magnitudes of these
flows, reveals the nearly decomposable structure
of the system—with one qualification. There is a
consumption subsystem of the economy that is
linked strongly to variables in most of the other
subsystems. Hence, we have to modify our no-
tions of decomposability slightly to accommo-
date the special role of the consumption subsys-
tem in our analysis of the dynamic behavior of
the economy.
In the dynamics of social systems, where mem-
bers of a system communicate with and influence
other members, near decomposability is generally
very prominent. This is most obvious in formal
organizations, where the formal authority relation
connects each member of the organization with
one immediate superior and with a small number
of subordinates. Of course, many communica-
tions in organizations follow other channels than
the lines of formal authority. But most of these
channels lead from any particular individual to a
very limited number of his superiors, subordi-
nates, and associates. Hence, departmental
boundaries play very much the same role as the
walls in our heat example.
NEAR DECOMPOSABILITY OF SOCIAL SYSTEMS PHYSICO-CHEMICAL SYSTEMS
As a glance at Figure 1 shows, near decom-
posability is a rather strong property for a matrix
to possess, and the matrices that have this prop-
erty will describe very special dynamic sys-
tems—vanishingly few systems out of all those
that are thinkable. How few they will be depends,
of course, on how good an approximation we
insist upon. If we demand that epsilon be very
small, correspondingly few dynamic systems will
fit the definition. But we have already seen that
in the natural world nearly decomposable sys-
tems are far from rare. On the contrary, systems
in which each variable is linked with almost
equal strength with almost all other parts of the
system are far rarer and less typical.
In economic dynamics, the main variables are
the prices and quantities of commodities. It is
empirically true that the price of any given com-
modity and the rate at which it is exchanged de-
pend to a significant extent only on the prices and
quantities of a few other commodities, together
with a few other aggregate magnitudes, like the
average price level or some over-all measure of
economic activity. The large linkage coefficients In the complex systems familar in biological
chemistry, a similar structure is clearly visible.
Take the atomic nuclei in such a system as the
elementary parts of the system, and construct a
matrix of bond strengths between elements.
There will be matrix elements of quite different
orders of magnitude. The largest will generally
correspond to the covalent bonds, the next to the
ionic bonds, the third group to hydrogen bonds,
still smaller linkages to van der Waals forces. 16 If
we select an epsilon just a little smaller than the
magnitude of a covalent bond, the system will
decompose into subsystems—the constituent
molecules. The smaller linkages will correspond
to the intermolecular bonds.
It is well known that high-energy, high-
frequency vibrations are associated with the
16
For a survey of the several classes of molecular and
intermolecular forces, and their dissociation energies, see
Setlow and Pollard, op. cit., Chapter 6. The energies of
typical covalent bonds are of the order of 80-100 k
cal/mole, of the hydrogen bonds, 10 k cal/mole. Ionic
bonds generally lie between these two levels; the bonds
due to van der Waals forces are lower in energy.VOL. 106, NO. 6, 1962]
THE ARCHITECTURE OF COMPLEXITY
smaller physical subsystems, low-frequency vi-
brations with the larger systems into which the
subsystems are assembled. For example, the ra-
diation frequencies associated with molecular
vibrations are much lower than those associated
with the vibrations of the planetary electrons of
the atoms; the latter, in turn, are lower than those
associated with nuclear processes. 17 Molecular
systems are nearly decomposable systems, the
short-run dynamics relating to the internal struc-
tures of the subsystems, the long-run dynamics to
the interactions of these subsystems.
A number of the important approximations em-
ployed in physics depend for their validity on the
near decom-posability of the systems studied.
The theory of the thermodynamics of irreversible
processes, for example, requires the assumption
of macroscopic disequilibrium but microscopic
equilibrium, 18 exactly the situation described in
our heat-exchange example. Similarly, computa-
tions in quantum mechanics are often handled by
treating weak interactions as producing perturba-
tions on a system of strong interactions.
SOME OBSERVATIONS ON HIERARCHIC SPAN
To understand why the span of hierarchies is
sometimes very broad—as in crystals—and
sometimes narrow, we need to examine more
detail of the interactions. In general, the critical
consideration is the extent to which interaction
between two (or a few) subsystems excludes in-
teraction of these subsystems with the others. Let
us examine first some physical examples.
Consider a gas of identical molecules, each of
which can form covalent bonds, in certain ways,
with others. Let us suppose that we can associate
with each atom a specific number of bonds that it
is capable of maintaining simultaneously. (This
number is obviously related to the number we
usually call its valence.) Now suppose that two
atoms join and that we can also associate with the
combination a specific number of external bonds
it is capable of maintaining. If this number is the
17
Typical wave numbers for vibrations associated with
various systems (the wave number is the reciprocal of
wave length hence proportional -10 to frequency):
steel wire under tension—10 to 10 -9 cm -1
molecular rotations—10 0 2 to 10 2 3 cm -1 -1
molecular vibrations—10 to 10 cm
4
planetary electrons—10
to 10 5 cm- 1
nuclear rotations—10 9 to 10 10 cm -1
nuclear
surface vibrations—10 11 to 10 12 cm -1 .
18
S. R. de Groot, Thermodynamics of irreversible
processes, 11-12, New York, Interscience Publishers,
1951.
476
same as the number associated with the individ-
ual atoms, the bonding process can go on indefi-
nitely—the atoms can form crystals or polymers
of indefinite extent. If the number of bonds of
which the composite is capable is less than the
number associated with each of the parts, then
the process of agglomeration must come to a halt.
We need only mention some elementary exam-
ples. Ordinary gases show no tendency to ag-
glomerate, because the multiple bonding of at-
oms “uses up” their capacity to interact. While
each oxygen atom has a valence of two, the O 2
molecules have a zero valence. Contrariwise, in-
definite chains of single-bonded carbon atoms
can be built up, because a chain of any number of
such atoms, each with two side groups, has a va-
lence of exactly two.
Now what happens if we have a system of ele-
ments that possess both strong and weak interac-
tion capacities and whose strong bonds are ex-
haustible through combination? Subsystems will
form, until all the capacity for strong interaction
is utilized in their construction. Then these sub-
systems will be linked by the weaker second-order
bonds into larger systems. For example, a water
molecule has essentially a valence of zero—all the
potential covalent bonds are fully occupied by
the interaction of hydrogen and oxygen molecules.
But the geometry of the molecule creates an elec-
tric dipole that permits weak interaction between
the water and salts dissolved in it— whence such
phenomena as its electrolytic conductivity. 19
Similarly, it has been observed that, although
electrical forces are much stronger than gravita-
tional forces, the latter are far more important
than the former for systems on an astronomical
scale. The explanation, of course, is that the elec-
trical forces, being bipolar, are all “used up” in
the linkages of the smaller subsystems, and that
significant net balances of positive or negative
charges are not generally found in regions of
macroscopic size.
In social as in physical systems there are gener-
ally limits on the simultaneous interaction of
large numbers of subsystems. In the social case,
these limits are related to the fact that a human
being is more nearly a serial than a parallel in-
formation-processing system. He can carry on
only one conversation at a time, and although this
does not limit the size of the audience to which a
mass communication can be addressed, it does
19
15.
See, for example, L. Pauling, General chemistry, ch.477
HERBERT A. SIMON
limit the number of people simultaneously involved
in most other forms of social interaction. Apart
from requirements of direct interaction, most roles
impose tasks and responsibilites that are time con-
suming. One cannot, for example, enact the role
of “friend” with large numbers of other people.
It is probably true that in social as in physical
systems, the higher-frequency dynamics are as-
sociated with the subsystems, the lower-
frequency dynamics with the larger systems. It is
generally believed, for example, that the relevant
planning horizon of executives is longer, the
higher their location in the organizational hierar-
chy. It is probably also true that both the average
duration of an interaction between executives and
the average interval between interactions are
greater at higher than at lower levels.
SUMMARY: NEAR DECOMPOSABILITY
We have seen that hierarchies have the prop-
erty of near decomposability. Intracomponent
linkages are generally stronger than intercompo-
nent linkages. This fact has the effect of separat-
ing the high-frequency dynamics of a hierar-
chy—involving the internal structure of the com-
ponents—from the low-frequency dynamics—
involving interaction among components. We
shall turn next to some important consequences
of this separation for the description and compre-
hension of complex systems.
THE DESCRIPTION OF COMPLEXITY
If you ask a person to draw a complex object—
such as a human face—he will almost always
proceed in a hierarchic fashion. 20 First he will
outline the face. Then he will add or insert fea-
tures: eyes, nose, mouth, ears, hair. If asked to
elaborate, he will begin to develop details for
each of the features—pupils, eyelids, lashes for
the eyes, and so on—until he reaches thelimitsof
his anatomical knowledge. His information about
the object is arranged hierarchically in memory,
like a topical outline.
When information is put in outline form, it is
easy to include information about the relations
among the major parts and information about the
internal relations of parts in each of the suboutli-
nes. Detailed information about the relations of
20
George A. Miller has collected protocols from sub-
jects who were given the task of drawing faces and finds
that they behave in the manner described here (private
communication). See also E. H. Gombrich, Art and illu-
sion, 291-296, New York, Pantheon Books, 1960.
[ PROC. AMER. PHIL. SOC
subparts belonging to different parts has no place
in the outline and is likely to be lost. The loss of
such information and the preservation mainly of
information about hierarchic order is a salient
characteristic that distinguishes the drawings of a
child or someone untrained in representation
from the drawing of a trained artist. (I am speak-
ing of an artist who is striving for representation.)
NEAR DECOMPOSABILITY AND COMPREHENSIBILITY
From our discussion of the dynamic properties
of nearly decomposable systems, we have seen
that comparatively little information is lost by
representing them as hierarchies. Subparts be-
longing to different parts only interact in an ag-
gregative fashion—the detail of their interaction
can be ignored. In studying the interaction of two
large molecules, generally we do not need to
consider in detail the interactions of nuclei of the
atoms belonging to the one molecule with the
nuclei of the atoms belonging to the other. In
studying the interaction of two nations, we do not
need to study in detail the interactions of each
citizen of the first with each citizen of the second.
The fact, then, that many complex systems
have a nearly decomposable, hierarchic structure
is a major facilitating factor enabling to us under-
stand, to describe, and even to “see” such sys-
tems and their parts. Or perhaps the proposition
should be put the other way round. If there are
important systems in the world that are complex
without being hierarchic, they may to a consider-
able extent escape our observation and our un-
derstanding. Analysis of their behavior would
involve such detailed knowledge and calculation
of the interactions of their elementary parts that it
would be beyond our capacities of memory or
computation. 21
21
I believe the fallacy in the central thesis of W. M.
Elsasser’s The physical foundation of biology, mentioned
earlier, lies in his ignoring the simplification in description
of complex systems that derives from their hierarchic
structure. Thus (p. 155): “If we now apply similar argu-
ments to the coupling of enzymatic reactions with the
substratum of protein molecules, we see that over a suffi-
cient period of time, the information corresponding to the
structural details of these molecules will be communi-
cated to the dynamics of the cell, to higher levels of or-
ganization as it were, and may influence such dynamics.
While this reasoning is only qualitative, it lends credence
to the assumption that in the living organism, unlike the
inorganic crystal, the effects of microscopic structure can-
not be simply averaged out; as time goes on this influ-
ence will pervade the behavior of the cell ‘at all levels.’”
But from our discussion of near decomposability, it
would appear that those aspects of microstructure thatVOL. 106, NO. 6, 1962]
THE ARCHITECTURE OF COMPLEXITY
I shall not try to settle which is chicken and
which is egg: whether we are able to understand
the world because it is hierarchic or whether it
appears hierarchic because those aspects of it
which are not elude our understanding and ob-
servation. I have already given some reasons for
supposing that the former is at least half the
truth—that evolving complexity would tend to be
hierarchic—but it may not be the whole truth.
SIMPLE DESCRIPTIONS OF COMPLEX SYSTEMS
One might suppose that the description of a
complex system would itself be a complex struc-
ture of symbols—and indeed, it may be just that.
But there is no conservation law that requires that
the description be as cumbersome as the object
described. A trivial example will show how a
system can be described economically. Suppose
the system is a two-dimensional array like this:
A B M N R S H I
C D O P T U J K
M N A B H I R S
O P C D J K T U
R S H I A B M N
T U J K C D O P
H I R S M N A B
J K T U O P C D
AB 
 MN 
Let us call the array 
 CD  a, the array  OP 
RS 
 HI 
m, the array 
 TU  r, and the array  JK  h. Let us
am 
 rh 
call the array 
 ma  w, and the array  hr  x. Then
wx 
the entire array is simply 
 xw  . While the origi-
nal structure consisted of 64 symbols, it requires
only 35 to write down its description:
wx
s = xw
am
rh
w = ma
x = hr
HI
AB
MN
RS
a = CD
m = OP
r = TU
h = JK
We achieve the abbreviation by making use of
the redundancy in the original structure. Since the
control the slow developmental aspects of organismic
dynamics can be separated out from the aspects that con-
trol the more rapid cellular metabolic processes. For this
reason we should not despair of unraveling the web of
causes. See also J. R. Platt’s review of Elsasser’s book in
Perspectives in biology and medicine 2: 243-245, 1959.
478
AB
pattern CD , for example, occurs four times in the
total pattern, it is economical to represent it by
the single symbol, a.
If a complex structure is completely unredun-
dant—if no aspect of its structure can be inferred
from any other—then it is its own simplest de-
scription. We can exhibit it, but we cannot de-
scribe it by a simpler structure. The hierarchic
structures we have been discussing have a high
degree of redundancy, hence can often be described
in economical terms. The redundancy takes a
number of forms, of which I shall mention three:
1. Hierarchic systems are usually composed of
only a few different kinds of subsystems, in vari-
ous combinations and arrangements. A familiar
example is the proteins, their multitudinous vari-
ety arising from arrangements of only twenty
different amino acids. Similarly, the ninety-odd
elements provide all the kinds of building blocks
needed for an infinite variety of molecules. Hence,
we can construct our description from a restricted
alphabet of elementary terms corresponding to
the basic set of elementary subsystems from
which the complex system is generated.
2. Hierarchic systems are, as we have seen, of-
ten nearly decomposable. Hence only aggregative
properties of their parts enter into the description
of the interactions of those parts. A generaliza-
tion of the notion of near decomposability might
be called the “empty world hypothesis”—most
things are only weakly connected with most other
things; for a tolerable description of reality only a
tiny fraction of all possible interactions needs to
be taken into account. By adopting a descriptive
language that allows the absence of something to
go unmentioned, a nearly empty world can be
described quite concisely. Mother Hubbard did
not have to check off the list of possible contents
to say that her cupboard was bare.
3. By appropriate “receding,” the redundancy
that is present but unobvious in the structure of a
complex system can often be made patent. The
commonest receding of descriptions of dynamic
systems consists in replacing a description of the
time path with a description of a differential law
that generates that path. The simplicity, that is,
resides in a constant relation between the state of
the system at any given time and the state of the
system a short time later. Thus, the structure of
the sequence 1 3 5 7 9 11 ... is most simply ex-
pressed by observing that each member is ob-
tained by adding 2 to the previous one. But this is
the sequence that Galileo found to describe the479
HERBERT A. SIMON
velocity at the end of successive time intervals of
a ball rolling down an inclined plane.
It is a familar proposition that the task of sci-
ence is to make use of the world’s redundancy to
describe that world simply. 1 shall not pursue the
general methodological point here, but I shall
instead take a closer look at two main types of
description that seem to be available to us in
seeking an understanding of complex systems. I
shall call these state description and process de-
scription, respectively.
STATE DESCRIPTIONS AND PROCESS DESCRIPTIONS
“A circle is the locus of all points equidistant
from a given point.” “To construct a circle, rotate
a compass with one arm fixed until the other arm
has returned to its starting point.” It is implicit in
Euclid that if you carry out the process specified
in the second sentence, you will produce an ob-
ject that satisfies the definition of the first. The
first sentence is a state description of a circle, the
second a process description.
These two modes of apprehending structures
are the warp and weft of our experience. Pictures,
blueprints, most diagrams, and chemical struc-
tural formulas are state descriptions. Recipes,
differential equations, and equations for chemical
reactions are process descriptions. The former
characterize the world as sensed; they provide the
criteria for identifying objects, often by modeling
the objects themselves. The latter characterize the
world as acted upon; they provide the means for
producing or generating objects having the de-
sired characteristics.
The distinction between the world as sensed
and the world as acted upon defines the basic
condition for the survival of adaptive organisms.
The organism must develop correlations between
goals in the sensed world and actions in the
world of process. When they are made conscious
and verbalized, these correlations correspond to
what we usually call means-end analysis. Given a
desired state of affairs and an existing state of
affairs, the task of an adaptive organism is to find
the difference between these two states and then
to find the correlating process that will erase the
difference. 22
Thus, problem solving requires continual trans-
lation between the state and process descriptions
of the same complex reality. Plato, in the Meno,
22
See H. A. Simon and A. Newell, Simulation of hu-
man thinking, in M. Greenberger, ed., Management and
the computer of the future, 95-114, esp. pp. 110 ff., New
York, Wiley, 1962.
[ PROC. AMER. PHIL. SOC
argued that all learning is remembering. He could
not otherwise explain how we can discover or
recognize the answer to a problem unless we al-
ready know the answer. 23 Our dual relation to the
world is the source and solution of the paradox.
We pose a problem by giving the state descrip-
tion of the solution. The task is to discover a se-
quence of processes that will produce the goal
state from an initial state. Translation from the
process description to the state description en-
ables us to recognize when we have succeeded.
The solution is genuinely new to us—and we do
not need Plato’s theory of remembering to ex-
plain how we recognize it.
There is now a growing body of evidence that
the activity called human problem solving is ba-
sically a form of means-end analysis that aims at
discovering a process description of the path that
leads to a desired goal. The general paradigm is:
given a blueprint, to find the corresponding rec-
ipe. Much of the activity of science is an applica-
tion of that paradigm: given the description of
some natural phenomena, to find the differential
equations for processes that will produce the
phenomena.
THE DESCRIPTION OF COMPLEXITY IN
SELF-REPRODUCING SYSTEMS
The problem of finding relatively simple descrip-
tions for complex systems is of interest not only
for an understanding of human knowledge of the
world but also for an explanation of how a com-
plex system can reproduce itself. In my discus-
sion of the evolution of complex systems, I touched
only briefly on the role of self-reproduction.
Atoms of high atomic weight and complex in-
organic molecules are witnesses to the fact that
the evolution of complexity does not imply self-
reproduction. If evolution of complexity from
simplicity is sufficiently probable, it will occur
repeatedly; the statistical equilibrium of the sys-
tem will find a large fraction of the elementary
particles participating in complex systems.
If, however, the existence of a particular com-
plex form increased the probability of the crea-
tion of another form just like it, the equilibrium
between complexes and components could be
greatly altered in favor of the former. If we have
a description of an object that is sufficiently clear
and complete, we can reproduce the object from
the description. Whatever the exact mechanism
23
The works of Plato, B. Jowett, trans., 3: 26-35 (New
York: Dial Press.VOL. 106, NO. 6, 1962]
THE ARCHITECTURE OF COMPLEXITY
of reproduction, the description provides us with
the necessary information.
Now we have seen that the descriptions of
complex systems can take many forms. In par-
ticular, we can have state descriptions, or we can
have process descriptions— blueprints or recipes.
Reproductive processes could be built around
either of these sources of information. Perhaps
the simplest possibility is for the complex system
to serve as a description of itself—a template on
which a copy can be formed. One of the most
plausible current theories, for example, of the
reproduction of deoxyribo-nucleic acid (DNA)
proposes that a DNA molecule, in the form of a
double helix of matching parts (each essentially a
“negative” of the other), unwinds to allow each
half of the helix to serve as a template on which a
new matching half can form.
On the other hand, our current knowledge of
how DNA controls the metabolism of the organ-
ism suggests that reproduction by template is
only one of the processes involved. According to
the prevailing theory, DNA serves as a template
both for itself and for the related substance ribo-
nucleic acid (RNA). RNA, in turn, serves as a
template for protein. But proteins—according to
current knowledge—guide the organism’s me-
tabolism not by the template method but by serv-
ing as catalysts to govern reaction rates in the
cell. While RNA is a blueprint for protein, pro-
tein is a recipe for metabolism. 24
ONTOGENY RECAPITULATES PHYLOGENY
The DNA in the chromosomes of an organism
contains some, and perhaps most, of the informa-
tion that is needed to determine its development
and activity. We have seen that, if current theo-
ries are even approximately correct, the informa-
tion is recorded not as a state description of the
organism but as a series of “instructions” for the
construction and maintenance of the organism
from nutrient materials. I have already used the
metaphor of a recipe; I could equally well com-
pare it with a computer program, which is also a
sequence of instructions, governing the construc-
tion of symbolic structures. Let me spin out some
of the consequences of the latter comparison.
24
C. B. Anfinsen, The molecular basis of evolution,
chs. 3 and 10, New York, Wiley, 1959, will qualify this
sketchy, oversimplified account. For an imaginative dis-
cussion of some mechanisms of process description that
could govern molecular structure, see H. H. Pattee, On
the origin of macromolecular sequences, Biophysical
Journal 1: 683-710, 1961.
480
If genetic material is a program—viewed in its
relation to the organism—it is a program with
special and peculiar properties. First, it is a self-
reproducing program; we have already considered
its possible copying mechanism. Second, it is a
program that has developed by Darwinian evolu-
tion. On the basis of our watchmaker’s argument,
we may assert that many of its ancestors were also
viable programs—programs for the subassemblies.
Are there any other conjectures we can make
about the structure of this program? There is a
well-known generalization in biology that is ver-
bally so neat that we would be reluctant to give it
up even if the facts did not support it: ontogeny
recapitulates phylogeny. The individual organ-
ism, in its development, goes through stages that
resemble some of its ancestral forms. The fact
that the human embryo develops gill bars and
then modifies them for other purposes is a famil-
iar particular belonging to the generalization. Bi-
ologists today like to emphasize the qualifica-
tions of the principle—that ontogeny recapitu-
lates only the grossest aspects of phylogeny, and
these only crudely. These qualifications should
not make us lose sight of the fact that the gener-
alization does hold in rough approximation—it
does summarize a very significant set of facts
about the organism’s development. How can we
interpret these facts?
One way to solve a complex problem is to re-
duce it to a problem previously solved—to show
what steps lead from the earlier solution to a so-
lution of the new problem. If, around the turn of
the century, we wanted to instruct a workman to
make an automobile, perhaps the simplest way
would have been to tell him how to modify a
wagon by removing the singletree and adding a
motor and transmission. Similarly, a genetic pro-
gram could be altered in the course of evolution
by adding new processes that would modify a
simpler form into a more complex one—to con-
struct a gastrula, take a blastula and alter it!
The genetic description of a single cell may,
therefore, take a quite different form from the
genetic description that assembles cells into a
multicelled organism. Multiplication by cell divi-
sion would require, as a minimum, a state de-
scription (the DNA, say), and a simple “interpre-
tive process”—to use the term from computer
language —that copies this description as a part
of the larger copying process of cell division. But
such a mechanism clearly would not suffice for
the differentiation of cells in development. It ap-
pears more natural to conceptualize that mecha-481
HERBERT A. SIMON
nism as based on a process description, and a
somewhat more complex interpretive process that
produces the adult organism in a sequence of
stages, each new stage in development representing
the effect of an operator upon the previous one.
It is harder to conceptualize the interrelation of
these two descriptions. Interrelated they must be,
for enough has been learned of gene-enzyme
mechanisms to show that these play a major role
in development as in cell metabolism. The single
clue we obtain from our earlier discussion is that
the description may itself be hierarchical, or
nearly decomposable, in structure, the lower lev-
els governing the fast, “high-frequency” dynam-
ics of the individual cell, the higher-level interac-
tions governing the slow, “low-frequency” dy-
namics of the developing multicellular organism.
There are only bits of evidence, apart from the
facts of recapitulation, that the genetic program is
organized in this way, but such evidence as exists
is compatible with this notion. 25 To the extent
that we can differentiate the genetic information
that governs cell metabolism from the genetic
information that governs the development of dif-
ferentiated cells in the multicellular organization,
we simplify enormously—as we have already
seen—our task of theoretical description. But 1
have perhaps pressed this speculation far enough.
The generalization that, in evolving systems
whose descriptions are stored in a process lan-
guage, we might expect ontogeny partially to
recapitulate phylogeny has applications outside
the realm of biology. It can be applied as readily,
for example, to the transmission of knowledge in
25
There is considerable evidence that successive genes
along a chromosome often determine enzymes control-
ling successive stages of protein syntheses. For a review
of some of this evidence, see P. E. Hartman, Transduc-
tion: a comparative review, in W. D. McElroy and B.
Glass (eds.), The chemical basis of heredity, Baltimore,
Johns Hopkins Press, 1957, at pp. 442-454. Evidence for
differential activity of genes in different tissues and at
different stages of development is discussed by J. G.
Gall, Chromosomal differentiation, in W. D. McElroy
and B. Glass (eds.), The chemical basis of development,
Baltimore, Johns Hopkins Press, 1958, at pp. 103-135.
Finally, a model very like that proposed here has been
independently, and far more fully, outlined by J. R. Platt,
A ‘book model’ of genetic information transfer in cells
and tissues, in M. Kasha and B. Pullman (eds.), Horizons
in biochemistry, New York, Academic Press, forthcom-
ing. Of course, this kind of mechanism is not the only
one in which development could be controlled by a proc-
ess description. Induction, in the form envisaged in Spe-
mann’s organizer theory, is based on process description,
in which metabolites in already formed tissue control the
next stages of development.
[ PROC. AMER. PHIL. SOC
the educational process. In most subjects, particu-
larly in the rapidly advancing sciences, the pro-
gress from elementary to advanced courses is to a
considerable extent a progress through the con-
ceptual history of the science itself. Fortunately,
the recapitulation is seldom literal—any more
than it is in the biological case. We do not teach
the phlogiston theory in chemistry in order later
to correct it. (I am not sure I could not cite exam-
ples in other subjects where we do exactly that.)
But curriculum revisions that rid us of the accu-
mulations of the past are infrequent and painful.
Nor are they always desirable—partial recapitula-
tion may, in many instances, provide the most
expeditious route to advanced knowledge.
SUMMARY: THE DESCRIPTION OF COMPLEXITY
How complex or simple a structure is depends
critically upon the way in which we describe it.
Most of the complex structures found in the
world are enormously redundant, and we can use
this redundancy to simplify their description. But
to use it, to achieve the simplification, we must
find the right representation.
The notion of substituting a process description
for a state description of nature has played a cen-
tral role in the development of modern science.
Dynamic laws, expressed in the form of systems
of differential or difference equations, have in a
large number of cases provided the clue for the
simple description of the complex. In the preced-
ing paragraphs I have tried to show that this
characteristic of scientific inquiry is not acciden-
tal or superficial. The correlation between state
description and process description is basic to the
functioning of any adaptive organism, to its ca-
pacity for acting purposefully upon its environ-
ment. Our present-day understanding of genetic
mechanisms suggests that even in describing it-
self the multi-cellular organism finds a process
description—a genetically encoded program—to
be the parsimonious and useful representation.
CONCLUSION
Our speculations have carried us over a rather
alarming array of topics, but that is the price we
must pay if we wish to seek properties common
to many sorts of complex systems. My thesis has
been that one path to the construction of a non-
trivial theory of complex systems is by way of a
theory of hierarchy. Empirically, a large propor-
tion of the complex systems we observe in nature
exhibit hierarchic structure. On theoretical
grounds we could expect complex systems to be
hierarchies in a world in which complexity had to
evolve from simplicity. In their dynamics, hierar-
chies have a property, near decompos-ability, that
greatly simplifies their behavior. Near decom-
posability also simplifies the description of a
complex system and makes it easier to under-
stand how the information needed for the devel-
opment or reproduction of the system can be
stored in reasonable compass.
In both science and engineering, the study of
“systems” is an increasingly popular activity. Its
popularity is more a response to a pressing need
for synthesizing and analyzing complexity than it
is to any large development of a body of knowl-
edge and technique for dealing with complexity.
If this popularity is to be more than a fad, neces-
sity will have to mother invention and provide
substance to go with the name. The explorations
reviewed here represent one particular direction
of search for such substance.
